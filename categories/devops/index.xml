<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Devops on /dev/ser</title>
    <link>https://devser.com.br/categories/devops/</link>
    <description>Recent content in Devops on /dev/ser</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <lastBuildDate>Tue, 11 Jun 2019 09:44:39 -0300</lastBuildDate>
    
	<atom:link href="https://devser.com.br/categories/devops/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Investigação de fatores de latência de rede em um cluster Kubernetes</title>
      <link>https://devser.com.br/posts/investigacao-de-fatores-de-latencia-de-rede-em-um-cluster-kubernetes/</link>
      <pubDate>Tue, 11 Jun 2019 09:44:39 -0300</pubDate>
      
      <guid>https://devser.com.br/posts/investigacao-de-fatores-de-latencia-de-rede-em-um-cluster-kubernetes/</guid>
      <description>Realizei testes para investigar a latência de rede do cluster Kubernetes da minha empresa.
Eu gostaria de responder algumas perguntas:
 Se eu acessar diretamente o nó que está rodando o pod, a latência será menor do que se eu acessar o mesmo serviço utilizando como bridge um segundo nó da rede? E se eu colocar um load balancer distribuindo a carga entre todos os nós da rede? Qual é o aumento na latência?</description>
    </item>
    
  </channel>
</rss>